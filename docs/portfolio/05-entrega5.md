---
title: "Validaci√≥n y Selecci√≥n de Modelos - Fill in the Blanks"
date: 2025-08-31
---

## Contexto
Se utilizar√° el dataset **Student Dropout and Academic Success** para el uso de *pipelines*, haciendo una evaluaci√≥n y validaci√≥n cruzada del modelo, con el objetivo de obtener m√©tricas de estabilidad.

## Objetivos
- Aprender a prevenir *data leakage* usando *pipelines*  
- Implementar validaci√≥n cruzada (*cross-validation*) robusta  
- Comparar m√∫ltiples modelos de forma sistem√°tica  
- Interpretar m√©tricas de estabilidad y selecci√≥n de modelos  

## Actividades (con tiempos estimados)
- Investigaci√≥n del dataset ‚Äî 15 min  
- Investigaci√≥n de los nuevos conceptos (KFold, cross_val_score, etc.) ‚Äî 25 min  
- Elaboraci√≥n del c√≥digo ‚Äî 45 min  
- An√°lisis de los resultados ‚Äî 35 min  
- Documentaci√≥n de los hallazgos ‚Äî 25 min  

## Desarrollo
Se comenz√≥ con una investigaci√≥n b√°sica de los componentes utilizados en las librer√≠as importadas, y se investig√≥ el caso de **Student Dropout and Academic Success** como dataset para tener una idea del contexto de los datos con los que trabajar√≠amos.

Posteriormente se empez√≥ la elaboraci√≥n del c√≥digo en Google Colab, primero importando las librer√≠as necesarias, e importando los datos del JSON de Kaggle de la misma forma que en la entrega anterior.

Por √∫ltimo, se comenz√≥ con la elaboraci√≥n del c√≥digo, empezando por la importaci√≥n de las librer√≠as con sus respectivos componentes:

```python hl_lines="2 6" linenums="1"
!pip install ucimlrepo

# Importar librer√≠as que vamos a usar
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Para validaci√≥n y selecci√≥n de modelos
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
# Para cargar datos desde UCI ML Repository
from ucimlrepo import fetch_ucirepo
from sklearn.metrics import accuracy_score, classification_report

print("Setup completo!")
```

Luego se obtuvieron los datos de los estudiantes, y se dividieron en features y targets (valores de Dropout, Enrolled y Graduate).
Se mapearon dichas categor√≠as a valores 0, 1, 2 respectivamente para facilitar su manejo, observamos la distribuci√≥n de las clases, mostramos las features, y finalmente revisamos caracter√≠sticas b√°sicas de la edad de enrollment.

```python hl_lines="2 6" linenums="1"
# Cargar dataset de estudiantes desde UCI
student_data = fetch_ucirepo(id=697)

# Preparar datos
X = student_data.data.features
y = student_data.data.targets

print("Dataset: Student Dropout and Academic Success")
print(f"Estudiantes: {X.shape[0]}, Caracter√≠sticas: {X.shape[1]}")
print(f"Objetivo: Predecir {len(y.columns)} variable(s)")

# Explorar variable objetivo
target_col = y.columns[0]  # Primera columna objetivo
y_series = y[target_col]
print(f"\nVariable objetivo: {target_col}")

# Mapear valores para mejor interpretaci√≥n
target_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}
y_mapped = y_series.map(target_mapping)

# Distribuci√≥n de clases
print("\nDistribuci√≥n de resultados acad√©micos:")
value_counts = y_mapped.value_counts()
for outcome, count in value_counts.items():
    percentage = (count / len(y_mapped)) * 100
    print(f"  {outcome}: {count} estudiantes ({percentage:.1f}%)")

# Ver algunas caracter√≠sticas
print(f"\nPrimeras caracter√≠sticas:")
print(X.columns.tolist()[:10], "...")

# Estad√≠sticas b√°sicas
print(f"\nAge at enrollment:")
if 'Age at enrollment' in X.columns:
    age_col = X['Age at enrollment']
    print(f"  Promedio: {age_col.mean():.1f} a√±os")
    print(f"  Rango: {age_col.min():.0f}-{age_col.max():.0f} a√±os")
```

Se prepar√≥ la variable objetivo como una serie simple.

```python hl_lines="2 6" linenums="1"
# Convertir strings a n√∫meros para sklearn
target_mapping = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}
reverse_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}

# Si y_series contiene strings, convertir a n√∫meros
if y_series.dtype == 'object':
    y_target = y_series.map(reverse_mapping)
else:
    y_target = y_series

X_features = X       # Features del dataset

print("Datos preparados para validaci√≥n:")
print(f"X shape: {X_features.shape}")
print(f"y shape: {y_target.shape}")
print(f"Clases √∫nicas: {sorted(y_target.unique())}")
print(f"Mapeo: {target_mapping}")
```

Se cre√≥ un pipeline para la validaci√≥n cruzada, usando un KFold b√°sico y un StratifiedKFold. Finalmente, se compararon ambas validaciones cruzadas y se gener√≥ un diagrama de caja, observando que StratifiedKFold es m√°s estable y recomendado.

```python hl_lines="2 6" linenums="1"
# === VALIDACI√ìN CRUZADA PARA ESTABILIDAD ===

print("üî¨ VALIDACI√ìN CRUZADA: ¬øQu√© tan estable es nuestro modelo?")

# 1. Crear pipeline robusto para usar en CV
pipeline_robust = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

print("Pipeline creado para validaci√≥n cruzada")

# 2. Crear KFold b√°sico
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 3. Evaluar con KFold usando cross_val_score
scores_kfold = cross_val_score(
    pipeline_robust, X_features, y_target, cv=kfold, scoring='accuracy'
)

print(f"\nKFOLD RESULTS:")
print(f"   Scores individuales: {scores_kfold}")
print(f"   Media: {scores_kfold.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_kfold.std():.4f}")
print(f"   Resultado: {scores_kfold.mean():.4f} ¬± {scores_kfold.std():.4f}")

# 4. Crear StratifiedKFold (mantiene proporci√≥n de clases)
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 5. Evaluar con StratifiedKFold
scores_stratified = cross_val_score(
    pipeline_robust, X_features, y_target, cv=stratified_kfold, scoring='accuracy'
)

print(f"\nSTRATIFIED KFOLD RESULTS:")
print(f"   Scores individuales: {scores_stratified}")
print(f"   Media: {scores_stratified.mean():.4f}")
print(f"   Desviaci√≥n est√°ndar: {scores_stratified.std():.4f}")
print(f"   Resultado: {scores_stratified.mean():.4f} ¬± {scores_stratified.std():.4f}")

# 6. Comparar estabilidad (menor desviaci√≥n = m√°s estable)
print(f"\nCOMPARACI√ìN DE ESTABILIDAD:")
if scores_stratified.std() < scores_kfold.std():
    print("   StratifiedKFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "StratifiedKFold"
else:
    print("   KFold es M√ÅS ESTABLE (menor variabilidad)")
    mejor_cv = "KFold"

print(f"   Recomendaci√≥n: Usar {mejor_cv} para este dataset")

# 7. Visualizar la distribuci√≥n de scores
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.boxplot([scores_kfold, scores_stratified], labels=['KFold', 'StratifiedKFold'])
plt.title('Distribuci√≥n de Scores - Validaci√≥n Cruzada')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)
plt.show()
```

Como ultima secci√≤n de codigo lo que se hizo fue una comparaci√≤n entre distintos modelos, en este caso entre:
- LogisticRegression
- RidgeClassifier
- RandomForestClassifier

Se evaluaron los valores obtenidos de la validaci√≤n cruzada usada en cada modelo para encontrar el mejor, utilizando tambien diagramas de caja para obtener una representaci√≤n m√†s visual, uno para la acurracy promedio de cada modelo, y otro para el promedio +- la desviaci√≤n estandar.

El ganador termino siendo el RandomForest.

```python hl_lines="2 6" linenums="1"
# === COMPETENCIA DE MODELOS ===

print("üèÜ TORNEO: ¬øCu√°l modelo funciona mejor para diagn√≥stico m√©dico?")

# 1. Definir candidatos (diferentes algoritmos)
models = {
    'Logistic Regression': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', LogisticRegression(max_iter=1000, random_state=42))
    ]),

    # 2. Ridge Classifier (regresi√≥n log√≠stica con regularizaci√≥n L2)
    'Ridge Classifier': Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', RidgeClassifier(alpha=1.0, random_state=42))
    ]),

    # 3. Random Forest (ensemble, no necesita escalado)
    'Random Forest': Pipeline([
        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
    ])
}

print(f"Modelos en competencia: {list(models.keys())}")

# 4. Evaluar cada modelo con validaci√≥n cruzada
print(f"\nEVALUANDO MODELOS CON 5-FOLD CV...")

results = {}
for name, model in models.items():
    print(f"   Evaluando {name}...")

    # Usar StratifiedKFold para mantener balance de clases
    scores = cross_val_score(
        model, X_features, y_target,
        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
        scoring='accuracy'
    )

    results[name] = scores

    print(f"   {name}: {scores.mean():.4f} ¬± {scores.std():.4f}")
    print(f"      Scores: {[f'{s:.3f}' for s in scores]}")

# 5. Encontrar el mejor modelo
print(f"\nRESULTADOS FINALES:")

# Encontrar modelo con mayor accuracy promedio
best_mean_score = 0
best_model_name = ""

for name, scores in results.items():
    if scores.mean() > best_mean_score:
        best_mean_score = scores.mean()
        best_model_name = name

print(f"GANADOR: {best_model_name}")
print(f"Score: {best_mean_score:.4f}")

# 6. An√°lisis detallado de estabilidad
print(f"\nAN√ÅLISIS DE ESTABILIDAD:")
for name, scores in results.items():
    stability = scores.std()

    if stability < 0.02:
        status = "MUY ESTABLE"
    elif stability < 0.05:
        status = "ESTABLE"
    else:
        status = "INESTABLE"

    print(f"   {name}: {status} (std: {stability:.4f})")

# 7. Visualizaci√≥n comparativa
plt.figure(figsize=(12, 6))

# Boxplot de distribuci√≥n de scores
plt.subplot(1, 2, 1)
plt.boxplot([results[name] for name in models.keys()],
           labels=[name.split()[0] for name in models.keys()])
plt.title('Distribuci√≥n de Accuracy por Modelo')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

# Barplot de medias con error bars
plt.subplot(1, 2, 2)
names = list(models.keys())
means = [results[name].mean() for name in names]
stds = [results[name].std() for name in names]

plt.bar(range(len(names)), means, yerr=stds, capsize=5)
plt.xticks(range(len(names)), [name.split()[0] for name in names])
plt.title('Accuracy Promedio ¬± Desviaci√≥n Est√°ndar')
plt.ylabel('Accuracy')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

Como ejercicio extra se us√≥ el mejor modelo (RandomForest) y se optimiz√≥ mediante b√∫squeda de hiperpar√°metros con GridSearchCV y RandomizedSearchCV, evaluados nuevamente con cross-validation, y observando los mejores hiperpar√°metros que ofrecieron.

```python hl_lines="2 6" linenums="1"
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score

# Seleccionar el mejor modelo de la competencia anterior
best_model_base = models[best_model_name]

print(f"Optimizando hiperpar√°metros para: {best_model_name}")

# Definir espacio de b√∫squeda de hiperpar√°metros
if 'Random Forest' in best_model_name:
    param_grid = {
        'classifier__n_estimators': [50, 100, 200],
        'classifier__max_depth': [None, 10, 20, 30],
        'classifier__min_samples_split': [2, 5, 10]
    }
elif 'Logistic' in best_model_name:
    param_grid = {
        'classifier__C': [0.1, 1, 10, 100],
        'classifier__max_iter': [1000, 2000]
    }
else:  # Ridge
    param_grid = {
        'classifier__alpha': [0.1, 1, 10, 100]
    }

# M√âTODO 1: GridSearchCV (b√∫squeda exhaustiva)
print("\nM√©todo 1: GridSearchCV (b√∫squeda exhaustiva)")
grid_search = GridSearchCV(
    best_model_base,
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Grid): {grid_search.best_params_}")
print(f"Mejor score (Grid): {grid_search.best_score_:.4f}")

# M√âTODO 2: RandomizedSearchCV (b√∫squeda aleatoria, m√°s eficiente)
print("\nM√©todo 2: RandomizedSearchCV (b√∫squeda aleatoria)")
random_search = RandomizedSearchCV(
    best_model_base,
    param_grid,
    n_iter=20,  # Solo 20 combinaciones aleatorias
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=1
)

random_search.fit(X_features, y_target)

print(f"Mejores par√°metros (Random): {random_search.best_params_}")
print(f"Mejor score (Random): {random_search.best_score_:.4f}")

# Comparar eficiencia
print(f"\nComparaci√≥n de eficiencia:")
print(f"GridSearch prob√≥: {len(grid_search.cv_results_['params'])} combinaciones")
print(f"RandomSearch prob√≥: {len(random_search.cv_results_['params'])} combinaciones")

# Evaluar modelo final optimizado
final_model = grid_search.best_estimator_
final_scores = cross_val_score(final_model, X_features, y_target, cv=5)
print(f"\nModelo final optimizado: {final_scores.mean():.4f} ¬± {final_scores.std():.4f}")
```

## Evidencias
![alt text](../assets/Entrega5Img0.png)
![alt text](../assets/Entrega5Img1.png)
![alt text](../assets/Entrega5Img2.png)
![alt text](../assets/Entrega5Img3.png)
![alt text](../assets/Entrega5Img4.png)
![alt text](../assets/Entrega5Img5.png)

[Collab](https://colab.research.google.com/drive/1LPVfz1k5r_PqFv78RmyuAdG03qsu6nAc?usp=sharing)

## Preguntas para investigar

***¬øCu√°ntas muestras y caracter√≠sticas tiene el dataset?***  
Tiene **4424 muestras** y **36 caracter√≠sticas**.

***¬øQu√© tipos de variables incluye? (demogr√°ficas, acad√©micas, socioecon√≥micas)***  
- **Acad√©micas:** Calificaciones, cursos, carrera, evaluaciones, etc.  
- **Demogr√°ficas:** Edad, sexo, estado civil, nacionalidad, etc.  
- **Socioecon√≥micas:** Ocupaci√≥n de padres.  

Lista de variables:  
`['Marital Status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', "Mother's qualification", "Father's qualification", "Mother's occupation", "Father's occupation", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']`

***¬øLas clases est√°n balanceadas o desbalanceadas?***  
Hay **3 clases** que est√°n desbalanceadas. Esto ocurre cuando una o varias clases en un conjunto de datos tienen una representaci√≥n significativamente menor que otras.

***¬øQu√© significan las 3 categor√≠as objetivo?***  
Representan el estado final del estudiante al terminar el curso. Son las categor√≠as que queremos predecir:  
- **Dropout**  
- **Enrolled**  
- **Graduate**

---

## ¬øQu√© significan las m√©tricas de validaci√≥n?

- **Cross-Validation:** T√©cnica que divide los datos en *k* partes para entrenar y evaluar m√∫ltiples veces.  
- **Accuracy promedio:** Estimaci√≥n media del rendimiento esperado en datos nuevos.  
- **Desviaci√≥n est√°ndar:** Indica qu√© tan consistente es el modelo entre diferentes divisiones de datos.  
- **StratifiedKFold:** Mantiene la proporci√≥n de clases en cada *fold*, especialmente importante en datasets desbalanceados.  

---

## ¬øCu√°ndo usar cada m√©todo?

- **GridSearchCV:** Cuando tienes pocos hiperpar√°metros y suficiente tiempo de c√≥mputo.  
- **RandomizedSearchCV:** Cuando tienes muchos hiperpar√°metros o poco tiempo disponible.  
- **Pipeline + SearchCV:** Siempre, ya que previene *data leakage* autom√°ticamente.  
- **cross_val_score:** En el resultado final, para validar que la optimizaci√≥n no caus√≥ sobreajuste (*overfitting*).  

---

## Reflexi√≥n

### **Preguntas:**

***¬øQu√© es data leakage y por qu√© es peligroso?***  
Es cuando el modelo accede a informaci√≥n de la parte de testing, o a *features* que no deber√≠an estar en el entrenamiento.  
Es peligroso porque el modelo puede sobreajustarse y, aunque muestre buen rendimiento en entrenamiento, luego falle con datos nuevos y desconocidos.  

***¬øCu√°ndo usar KFold vs StratifiedKFold?***  
- Si las clases est√°n balanceadas ‚Üí usar **KFold**.  
- Si hay clases desbalanceadas ‚Üí usar **StratifiedKFold**, ya que mantiene la proporci√≥n de clases en cada fold.  

***¬øC√≥mo interpretar "95.2% ¬± 2.1%" en cross-validation?***  
- **95.2%** es el *accuracy promedio*.  
- **¬± 2.1%** es la variabilidad entre los *folds*.  
Esto indica buena precisi√≥n promedio y resultados consistentes entre folds.  

***¬øPor qu√© Random Forest no necesita StandardScaler?***  
Porque los √°rboles de decisi√≥n se basan en comparaciones de valores y divisiones en categor√≠as, por lo que la normalizaci√≥n no afecta los resultados.  

***En diagn√≥stico m√©dico, ¬øprefieres un modelo con 98% accuracy pero inestable, o 95% accuracy pero muy estable?***  
Prefiero el **95% pero muy estable**, ya que en medicina la confiabilidad es m√°s importante que una mayor precisi√≥n aislada. Las predicciones deben ser lo m√°s seguras posibles para evitar errores graves en pacientes.  

## Referencias
- https://juanfkurucz.com/ucu-ia/ut1/05-validacion-seleccion-modelos/
- https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success
- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html
- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html
- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html
- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html