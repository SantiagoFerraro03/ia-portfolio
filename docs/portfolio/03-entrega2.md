---
title: "Feature Engineering y modelo base"
date: 2025-08-31
---

## Contexto
Siguiendo con el caso del titanic, se trabaja con feature engineering y un modelo base, aprendiendo el uso de baselines, preparaci√≥n de datos, etc.

## Objetivos
- Investigar componentes de scikit-learn
- Aprender que es y como aplicar feature engineering
- Aprender que es el modelo base, y base line, y aplicarlos
- Evaluar nuevas metricas con la aplicaci√≥n de los conceptos previamente mencionados.

## Actividades (con tiempos estimados)
- Investigaci√≥n del caso ‚Äî 5 min  
- Investigaci√≥n de los componentes ‚Äî 30 min  
- Elaboraci√≥n del c√≥digo ‚Äî 20 min  
- An√°lisis de los resultados ‚Äî 45 min  
- Documentaci√≥n de los hallazgos ‚Äî 20 min  

## Desarrollo
Inicialmente se refresco el caso brevemente, y se investigaron ciertos componentes de scikit-learn que se usarian a lo largo del codigo, a partir de la documentaci√≥n oficial.

Posteriormente se empezo la elaboraci√≥n del codigo en google collab, primero importando las librerias necesarias, e importando los datos del JSON de Kaggle de la misma forma que en la anterior entrega.

```python hl_lines="2 6" linenums="1"
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette('deep')

from pathlib import Path
try:
    from google.colab import drive
    drive.mount('/content/drive')
    ROOT = Path('/content/drive/MyDrive/IA-UT1')
except Exception:
    ROOT = Path.cwd() / 'IA-UT1'

DATA_DIR = ROOT / 'data'
RESULTS_DIR = ROOT / 'results'
for d in (DATA_DIR, RESULTS_DIR):
    d.mkdir(parents=True, exist_ok=True)
print('Outputs ‚Üí', ROOT)

!pip -q install kaggle
from google.colab import files
files.upload()  # Sub√≠ tu archivo kaggle.json descargado
!mkdir -p ~/.kaggle && cp kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c titanic -p data
!unzip -o data/titanic.zip -d data

train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
```

Luego se comenzo primero con el manejo de los valores faltantes, al hacer la imputaci√≥n. De Embarked para los valores faltantes se relleno con el valor mas comun, para Fare se uso la mediana, y para Age la mediana del sexo, y Pclass a la que perteneciera el record.

```python hl_lines="2 6" linenums="1"
df = train.copy()

# üö´ PASO 1: Manejar valores faltantes (imputaci√≥n)
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])  # Valor m√°s com√∫n
df['Fare'] = df['Fare'].fillna(df['Fare'].median())              # Mediana
df['Age'] = df['Age'].fillna(df.groupby(['Sex','Pclass'])['Age'].transform('median'))

# üÜï PASO 2: Crear nuevas features √∫tiles
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['IsAlone'] = (df['FamilySize'] == 1).astype(int)

df['Title'] = df['Name'].str.extract(',\s*([^\.]+)\.')
rare_titles = df['Title'].value_counts()[df['Title'].value_counts() < 10].index
df['Title'] = df['Title'].replace(rare_titles, 'Rare')

# üîÑ PASO 3: Preparar datos para el modelo
features = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','IsAlone','Title','SibSp','Parch']
X = pd.get_dummies(df[features], drop_first=True)
y = df['Survived']

X.shape, y.shape
```

Finalmente se creo la baseline para tener una base con la que comparar nuestro modelo, ya que nos permite saber si realmente es efectivo haciendo predicciones o no. Para la baseline se uso un DummyClassifier con la estrategia de most_frequent, y para el modelo se uso regresi√≥n lineal.

Por ultimo se imprimieron las accuracy de tanto el baseline como el del modelo para compararlas, la matriz de confusion, y el reporte de clasificaci√≥n.

```python hl_lines="2 6" linenums="1"
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

dummy = DummyClassifier(strategy='most_frequent', random_state=42)
dummy.fit(X_train, y_train)
baseline_pred = dummy.predict(X_test)

lr = LogisticRegression(max_iter=1000, solver='liblinear', random_state=42)
lr.fit(X_train, y_train)
pred = lr.predict(X_test)

print('Baseline acc:', accuracy_score(y_test, baseline_pred))
print('LogReg acc  :', accuracy_score(y_test, pred))

print('\nClassification report (LogReg):')
print(classification_report(y_test, pred))

print('\nConfusion matrix (LogReg):')
print(confusion_matrix(y_test, pred))
```

## Evidencias
![alt text](../assets/Entrega2Img1.png)

[Collab](https://colab.research.google.com/drive/1lEsXMWq7gRSv3R0i90mMkNZ8TXO_BH0v?usp=sharing)

## Investigaci√≥n de componentes Scikit-learn

---

### LogisticRegression

***¬øQu√© tipo de problema resuelve?***  
Implementa un modelo de regresi√≥n lineal que nos sirve como clasificador.

***¬øQu√© par√°metros importantes tiene?***  
Par√°metros:  
`penalty, dual, tol, C, fit_intercept, intercept_scaling, class_weight, random_state, solver, max_iter, multi_class, verbose, warm_start, n_jobs, l1_ratio`

***¬øCu√°ndo usar solver='liblinear' vs otros solvers?***  
Cuando el dataset es peque√±o es una buena opci√≥n. Para datasets m√°s grandes es mejor usar otros solvers m√°s eficientes como `saga` y `sag`.

---

### DummyClassifier

***¬øPara qu√© sirve exactamente?***  
Sirve como base para poder ser usado en comparaciones con otros clasificadores m√°s complejos. Hace predicciones ignorando los inputs.

***¬øQu√© estrategias de baseline ofrece?***  
- **most_frequent:** Devuelve la clase m√°s frecuente, como un vector.  
- **prior:** Devuelve la clase m√°s frecuente, como valor de la distribuci√≥n.  
- **stratified:** Devuelve valores aleatorios.  
- **uniform:** Devuelve valores de forma uniforme a partir de una lista de clases.  
- **constant:** Devuelve siempre el mismo valor indicado por el usuario.  

***¬øPor qu√© es importante tener un baseline?***  
Sirve para ver si tu modelo en verdad es √∫til, o no sirve para nada. Lo comparas con esa baseline a partir del dummy para verificar si tu modelo hace predicciones mejores que este.

---

### train_test_split

***¬øQu√© hace el par√°metro stratify?***  
Si no es `None`, divide los datos conservando la proporci√≥n de clases seg√∫n el vector `y`.

***¬øPor qu√© usar random_state?***  
Establece una semilla para que la partici√≥n sea reproducible en distintos experimentos.

***¬øQu√© porcentaje de test es recomendable?***  
Por defecto se usa un **25%** si no se indica en el par√°metro `test_size` o `train_size`, pero por lo general se emplea entre **20% y 30%**.

---

### M√©tricas de evaluaci√≥n

***¬øQu√© significa cada m√©trica en classification_report?***  
Para cada clase muestra: precisi√≥n, recall, f1-score y support.  
Tambi√©n incluye:  
- **accuracy (global)**  
- **macro avg:** promedio simple por clase  
- **weighted avg:** promedio ponderado por soporte (support)  
- **micro avg (en algunos casos):** promedio global de todas las clases sin ponderaci√≥n  

**Definiciones:**  
- **Precisi√≥n:** de todo lo que predijiste como positivo, ¬øcu√°ntos fueron realmente positivos?  
- **Recall:** de todos los positivos reales, ¬øcu√°ntos encontraste?  
- **F1-score:** media arm√≥nica de precisi√≥n y recall.  
- **Support:** cantidad de muestras reales de cada clase.  
- **Accuracy:** precisi√≥n global, es decir, del total de predicciones cu√°ntas fueron correctas.  

***¬øC√≥mo interpretar la matriz de confusi√≥n?***  
Permite ver los verdaderos positivos, falsos positivos, falsos negativos y verdaderos negativos entre clases.

***¬øCu√°ndo usar accuracy vs otras m√©tricas?***  
- Si la clase est√° balanceada y solo importa predecir el mayor n√∫mero de valores correctos, **accuracy** es la mejor m√©trica.  
- Si es importante tener en cuenta falsos positivos o falsos negativos (ejemplo: detecci√≥n de enfermedades), puede ser m√°s significativo usar **precisi√≥n** o **recall**.  
- Si lo que interesa es un balance entre la detecci√≥n de positivos y negativos, lo ideal es usar **f1-score**.  

## Reflexi√≥n

### **Preguntas:**

**Matriz de confusi√≥n**: ¬øEn qu√© casos se equivoca m√°s el modelo: cuando predice que una persona sobrevivi√≥ y no lo hizo, o al rev√©s?

**Respuesta:** Que predijera que s√≠ sobrevivi√≥ y en la realidad no sobreviviera es la celda [0,1] con valor 12, mientras que predecir que no sobrevivi√≥ pero s√≠ sobrevivi√≥ en la realidad es la celda [1,0] con valor 21. Como 21 > 12, podemos afirmar que se equivoc√≥ m√°s al decir que la persona no sobreviviera cuando en realidad s√≠ sobrevivi√≥.

---

**Clases atendidas**: ¬øEl modelo acierta m√°s con los que sobrevivieron o con los que no sobrevivieron?

**Respuesta:** Tanto la precisi√≥n como el recall son mayores en la fila 0 que en la 1, como se puede ver en el Classification Report, por lo que el modelo predice mejor a los que no sobrevivieron (0.82 > 0.80 y 0.89 > 0.70).

---

**Comparaci√≥n con baseline**: ¬øLa Regresi√≥n Log√≠stica obtiene m√°s aciertos que el modelo que siempre predice la clase m√°s com√∫n?

**Respuesta:** Verdadero, dado que se puede observar que el accuracy de la regresi√≥n log√≠stica es mayor que el del baseline: 0.8156 > 0.6145.

---

**Errores m√°s importantes**: ¬øCu√°l de los dos tipos de error cre√©s que es m√°s grave para este problema?

**Respuesta:** Es m√°s grave tener falsos negativos de personas que sobrevivieron, dado que si se usaran estos datos para rescatarlas, ser√≠a mejor captar a m√°s personas que pudieron haber sobrevivido aunque al final no lo hicieran, que simplemente minimizar el n√∫mero total de errores.

---

**Observaciones generales**: Mirando las gr√°ficas y n√∫meros, ¬øqu√© patrones interesantes encontraste sobre la supervivencia?

**Respuesta:** Podemos observar que hay m√°s datos de soporte de las personas que no sobrevivieron que de las que s√≠. Adem√°s, se suele predecir mejor la no supervivencia, hay m√°s falsos negativos que falsos positivos, y considero que las features Age, Sex y Pclass son las m√°s relevantes y ayudan a que el modelo pueda predecir con un accuracy del 82%.

---

**Mejoras simples**: ¬øQu√© nueva columna (feature) se te ocurre que podr√≠a ayudar a que el modelo acierte m√°s?

**Respuesta:** Una posible mejora ser√≠a usar grupos de edades en vez de la edad directamente, ya que la edad por s√≠ sola puede no capturar patrones tan claros, mientras que las agrupaciones podr√≠an ser m√°s √∫tiles para predecir la supervivencia con regresi√≥n log√≠stica.

## Referencias
- https://juanfkurucz.com/ucu-ia/ut1/02-feature-modelo-base/
- https://www.kaggle.com/competitions/titanic/data
- https://scikit-learn.org/stable/user_guide.html
